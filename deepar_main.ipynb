{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_forecasting import DeepAR, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import MultivariateDistributionLoss\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score, \\\n",
    "    explained_variance_score, accuracy_score, precision_score\n",
    "\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_delimiter(file_path, bytes=4096):\n",
    "    \"\"\"\n",
    "    Retrieves the delimiter of a csv file.\n",
    "    Args:\n",
    "        file_path: path to csv file to read\n",
    "        bytes: n bytes to read to detect the delimiter (higher is more guaranteed accuracy)\n",
    "\n",
    "    Returns:\n",
    "        delimiter: delimiter of the csv file located in the given path\n",
    "\n",
    "    \"\"\"\n",
    "    sniffer = csv.Sniffer()\n",
    "    data = open(file_path, \"r\").read(bytes)\n",
    "    delimiter = sniffer.sniff(data).delimiter\n",
    "    return delimiter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_csvs_from_folder(folder_path, bytes=4096, list=False):\n",
    "    \"\"\"\n",
    "    Reads all CSV files in a folder and stores them as pandas DataFrames in a list.\n",
    "    Automatically detects the delimiter for each CSV file.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        bytes (int): Number of bytes to read from each file for delimiter detection.\n",
    "\n",
    "    Returns:\n",
    "        Concatenated pandas DataFrame containing each CSV file in the folder.\n",
    "    \"\"\"\n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    # Read each CSV file into a pandas DataFrame and store them in a list\n",
    "    dataframes = []\n",
    "    for file in csv_files:\n",
    "        delimiter = get_delimiter(file, bytes)\n",
    "        df = pd.read_csv(file, delimiter=delimiter)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    if not list:\n",
    "        dataframes = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "    return dataframes\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def query_yf(stock: tuple, period: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query the Yahoo Finance API for a given stock and store the data.\n",
    "\n",
    "    :param stock: tuple\n",
    "        A tuple containing three elements:\n",
    "        - company (str): The name of the company.\n",
    "        - ticker (str): The stock ticker symbol of the company.\n",
    "        - exchange (str): The stock exchange where the company is listed.\n",
    "    :param period: int\n",
    "        The number of days of data to retrieve.\n",
    "\n",
    "    :return: data: a pandas dataframe of the data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract company, ticker, and exchange from the stock tuple\n",
    "        company, ticker, exchange = stock\n",
    "\n",
    "        yesterday = datetime.now() - timedelta(days=period)\n",
    "        today = datetime.now()\n",
    "\n",
    "        yesterday_str = yesterday.strftime('%Y-%m-%d')\n",
    "        today_str = today.strftime('%Y-%m-%d')\n",
    "        today_str = '2023-12-18'\n",
    "\n",
    "        # Query the API using yfinance\n",
    "        stock_data = yf.Ticker(ticker)\n",
    "        data = stock_data.history(period='1d', start=yesterday_str, end=today_str)\n",
    "\n",
    "        # Add the company name to the dataframe\n",
    "        data['Company'] = company\n",
    "        data['Ticker'] = ticker\n",
    "        data['Exchange'] = exchange\n",
    "\n",
    "        # Reorder the columns\n",
    "        data = data[['Company', 'Ticker', 'Exchange', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "        # Convert DataFrame index to a 'DatetimeIndex' without time zone information\n",
    "        data.index = data.index.tz_localize(None)  # this is necessary for some algorithms\n",
    "\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data: {e}\")\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def query_yf_list(stocks: pd.DataFrame, period: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query the Yahoo Finance API for all stocks in a list based on their exchange.\n",
    "\n",
    "    :param stocks: list\n",
    "        A list of tuples containing three elements:\n",
    "        - company (str): The name of the company.\n",
    "        - ticker (str): The stock ticker symbol of the company.\n",
    "        - exchange (str): The stock exchange where the company is listed.\n",
    "    :param period: int\n",
    "        The number of days of data to retrieve.\n",
    "\n",
    "    :return: data: a list of pandas dataframes of the data\n",
    "    \"\"\"\n",
    "    # Create an empty list\n",
    "    data = []\n",
    "\n",
    "    # Loop through the list of stocks\n",
    "    for _, row in stocks.iterrows():\n",
    "        stock = (row['Company'], row['Ticker'], row['Stock Exchange'])\n",
    "\n",
    "        # Query the API using yfinance\n",
    "        stock_data = query_yf(stock, period)\n",
    "\n",
    "        # Append the data to the dataframe\n",
    "        if stock_data is not None:\n",
    "            data.append(stock_data)\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_stocks_to_csv(data_list: [pd.DataFrame], folder_path: str = \"stock_data\"):\n",
    "    \"\"\"\n",
    "    Save a list of stock dataframes to a folder, each as a separate CSV file {company_name} data.csv.\n",
    "    :param data_list: list of dataframes to save\n",
    "    :param folder_path: folder path to save csvs to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        for df in data_list:\n",
    "            df.reset_index(inplace=True)\n",
    "            company = df.iloc[0]['Company']\n",
    "            df.to_csv(os.path.join(folder_path, f\"{company} data.csv\"), index=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data: {e}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def day_average(df: pd.DataFrame, column: str = 'Close', days: int = 30, company_name: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the n-day average of the stock price.\n",
    "    :param column: column to average\n",
    "    :param df: dataframe of stock data\n",
    "    :param days: number of days to average\n",
    "    :return: dataframe of stock data with added column for an n day average\n",
    "    \"\"\"\n",
    "    if company_name is not None:\n",
    "        column_title = f\"{company_name}-{column} Day Average\"\n",
    "    else:\n",
    "        column_title = f\"{column} Day Average\"\n",
    "    df[column_title] = df[column].rolling(days).mean().shift(1)  # shift to not include today\n",
    "    day_mean = df[column].iloc[0:days + 1].mean()\n",
    "    df[column_title] = df[column_title].fillna(day_mean)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def previous_day(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add the previous day's data to the dataframe as columns.\n",
    "    :param df: dataframe of stock data\n",
    "    :return: dataframe of stock data with added column for previous day's closing price\n",
    "    \"\"\"\n",
    "    df['Previous Close'] = df['Close'].shift(1)\n",
    "    df['Previous Close'].iloc[0] = df['Close'].iloc[0]\n",
    "\n",
    "    df['Previous Open'] = df['Open'].shift(1)\n",
    "    df['Previous Open'].iloc[0] = df['Open'].iloc[0]\n",
    "\n",
    "    df['Previous High'] = df['High'].shift(1)\n",
    "    df['Previous High'].iloc[0] = df['High'].iloc[0]\n",
    "\n",
    "    df['Previous Low'] = df['Low'].shift(1)\n",
    "    df['Previous Low'].iloc[0] = df['Low'].iloc[0]\n",
    "\n",
    "    df['Previous Volume'] = df['Volume'].shift(1)\n",
    "    df['Previous Volume'].iloc[0] = df['Volume'].iloc[0]\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def direction_evaluation(df: pd.DataFrame, truth_column: str, predicted_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column to the dataframe indicating whether the predicted direction (up or down) matches the actual subsequent movement\n",
    "    from the previous day's price to today's price.\n",
    "\n",
    "    :param df: dataframe of data with actual and predicted price columns\n",
    "    :param truth_column: string for the title of the truth column (actual price)\n",
    "    :param predicted_column: string for the title of the predicted price column\n",
    "    :return: dataframe with an additional column indicating if the predicted direction is correct\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Determine the actual direction by comparing today's actual price to yesterday's actual price\n",
    "    df['Actual Direction'] = np.where(df[truth_column] > df[truth_column].shift(1), 'Up', 'Down')\n",
    "    # Determine the predicted direction by comparing today's predicted price to yesterday's actual price\n",
    "    df['Predicted Direction'] = np.where(df[predicted_column] > df[truth_column].shift(1), 'Up', 'Down')\n",
    "    # Compare the actual direction to the predicted direction\n",
    "    df['Direction Correct'] = np.where(df['Actual Direction'] == df['Predicted Direction'], 'Correct', 'Incorrect')\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def simulate_trading(df: pd.DataFrame, initial_funds: int = 10000, truth_column: str = 'Close', predicted_column: str = 'Predicted Close') -> (float, float, float):\n",
    "    \"\"\"\n",
    "    Simulate trading based on predictions to calculate profitability and average profit per trade.\n",
    "\n",
    "    :param df: dataframe of data with actual and predicted price columns\n",
    "    :param initial_funds: initial investment amount\n",
    "    :param truth_column: string for the title of the truth column (actual price)\n",
    "    :param predicted_column: string for the title of the predicted price column\n",
    "    :return: net gain or loss from trading strategy, average profit per trade, net gain or loss percent\n",
    "    \"\"\"\n",
    "    funds = initial_funds\n",
    "    shares = 0\n",
    "    trades = []\n",
    "\n",
    "    df = df.copy()\n",
    "    df['Predicted Tomorrow'] = df[predicted_column].shift(-1)  # shift predictions to align with the day they are for\n",
    "\n",
    "    for i in range(len(df) - 1):  # minus 1 because the last day's prediction is for a day outside of our dataframe\n",
    "        today_price = df[truth_column].iloc[i]\n",
    "        predicted_tomorrow_price = df['Predicted Tomorrow'].iloc[i]\n",
    "\n",
    "        if not np.isnan(predicted_tomorrow_price) or not np.isnan(today_price):\n",
    "            # If the predicted price for tomorrow is higher than today's price, buy\n",
    "            if predicted_tomorrow_price > today_price:\n",
    "                shares_bought = funds // today_price\n",
    "                funds -= shares_bought * today_price\n",
    "                shares += shares_bought\n",
    "\n",
    "            # The next day, sell all shares if any were bought\n",
    "            if shares > 0:\n",
    "                next_day_price = df[truth_column].iloc[i + 1]\n",
    "                trade_profit = shares * (next_day_price - today_price)  # calculate profit for this trade\n",
    "                funds += shares * next_day_price\n",
    "                shares = 0  # reset shares to 0 after selling\n",
    "                trades.append(trade_profit)  # keep track of the profit from this trade\n",
    "\n",
    "    net_gain = funds - initial_funds\n",
    "    if not trades:\n",
    "        average_profit = 0\n",
    "    else:\n",
    "        average_profit = sum(trades) / len(trades)\n",
    "    net_gain_percent = net_gain / initial_funds * 100\n",
    "\n",
    "    return net_gain, average_profit, net_gain_percent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_standard_metrics(truth_series: pd.Series, predicted_series: pd.Series) -> (float, float, float, float, float, float):\n",
    "    \"\"\"\n",
    "    Calculate standard metrics for evaluating a trading strategy.\n",
    "    :param truth_series: series of actual prices\n",
    "    :param predicted_series: series of predicted prices\n",
    "    :return: RMSE, MAE, MAPE, Rsq, Explained Variance\n",
    "    \"\"\"\n",
    "    rmse = mean_squared_error(truth_series, predicted_series, squared=False)\n",
    "    mae = mean_absolute_error(truth_series, predicted_series)\n",
    "    mape = mean_absolute_percentage_error(truth_series, predicted_series)\n",
    "    rsq = r2_score(truth_series, predicted_series)\n",
    "    ev = explained_variance_score(truth_series, predicted_series)\n",
    "    return rmse, mae, mape, rsq, ev"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_accuracy(df: pd.DataFrame, truth_column: str = 'Close', predicted_column: str = 'Predicted Close') -> (float, float):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of a trading strategy by comparing the predicted direction to the actual direction.\n",
    "    :param df: dataframe of data with actual and predicted price columns\n",
    "    :param truth_column: column containing the actual price\n",
    "    :param predicted_column: column containing the predicted price\n",
    "    :return: accuracy, precision\n",
    "    \"\"\"\n",
    "    df = direction_evaluation(df, truth_column, predicted_column)\n",
    "    accuracy = accuracy_score(df['Actual Direction'], df['Predicted Direction'])\n",
    "    precision = precision_score(df['Actual Direction'], df['Predicted Direction'], pos_label='Up')\n",
    "    return accuracy, precision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_to_ETF(urnm: pd.DataFrame, df: pd.DataFrame = None, window: int = 100, initial_funds: int = 10000):\n",
    "    \"\"\"\n",
    "    Compare the profitability of a trading strategy to the profitability of investing in the S&P 500 URNM ETF.\n",
    "    :param urnm: dataframe of URNM data\n",
    "    :param df: dataframe of data with actual and predicted price columns for a stock\n",
    "    :param truth_column: column containing the actual price\n",
    "    :param predicted_column: column containing the predicted price\n",
    "    :return: metrics of using the ETF as a trading strategy\n",
    "\n",
    "    Note this can be done in two ways:\n",
    "        1. Investing into the ETF and holding.\n",
    "        2. Using the ETF as a trading strategy.\n",
    "    This function will do the first option.\n",
    "    \"\"\"\n",
    "    urnm_copy = urnm.copy()\n",
    "    if df is not None:\n",
    "        start_date = df['Date'].iloc[0]\n",
    "        end_date = df['Date'].iloc[-1]\n",
    "        urnm = urnm[(urnm['Date'] >= start_date) & (urnm['Date'] <= end_date)]\n",
    "    else:\n",
    "        start_date = urnm['Date'].iloc[-window]\n",
    "        end_date = urnm['Date'].iloc[-1]\n",
    "\n",
    "    # option one, invest and Hold\n",
    "    start_price = urnm['Close'].iloc[0]\n",
    "    shares = initial_funds // start_price\n",
    "\n",
    "    end_price = urnm['Close'].iloc[-1]\n",
    "    funds = shares * end_price\n",
    "\n",
    "    hold_net_gain = funds - initial_funds\n",
    "    net_gain_percent = hold_net_gain / initial_funds * 100\n",
    "\n",
    "    return hold_net_gain, net_gain_percent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def summarize_metrics(folder_path: str, model: str):\n",
    "    \"\"\"\n",
    "    Summarize the metrics of each stock in the list.\n",
    "    :param folder_path: str\n",
    "        Path to the folder containing CSV files of the event horizon metrics for each stock.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    metrics_data = read_csvs_from_folder(folder_path)\n",
    "    event_horizons = metrics_data['Event Horizon'].unique()\n",
    "    print(\"unique event horizons: \", event_horizons)\n",
    "    event_horizon_dfs = {eh: metrics_data[metrics_data['Event Horizon'] == eh] for eh in event_horizons}\n",
    "\n",
    "    summary_data = []\n",
    "    for horizon, df in event_horizon_dfs.items():\n",
    "        summary = {\n",
    "            \"Event Horizon\": horizon,\n",
    "            \"Average RMSE\": df['RMSE'].mean(),\n",
    "            \"Average MAPE\": df['MAPE'].mean(),\n",
    "            \"Average MAE\": df['MAE'].mean(),\n",
    "            \"Average RSquared\": df['R2'].mean(),\n",
    "            \"Average Accuracy\": df['Accuracy'].mean(),\n",
    "            \"Average Precision\": df['Precision'].mean(),\n",
    "            \"Average Net Gain\": df['Net Gain'].mean(),\n",
    "            \"Average Net Gain %\": df['Net Gain %'].mean(),\n",
    "            \"Average Profit\": df['Avg Profit'].mean()\n",
    "        }\n",
    "        summary_data.append(summary)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    save_path = os.path.join(os.getcwd(), f'{model} Summary.csv')\n",
    "    summary_df.to_csv(save_path, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_deepar(data: pd.DataFrame, max_prediction_length: int, max_encoder_length: int) -> [dict]:\n",
    "    \"\"\"\n",
    "    Runs the DeepAR algorithm on the input DataFrame and returns a list of dictionaries containing the metrics for each event horizon.\n",
    "\n",
    "    :param df: The input DataFrame containing stock market data. It must include the columns:\n",
    "    'Date', 'Company', 'Ticker', 'Exchange', 'Open', 'High', 'Low', 'Close', and 'Volume'.\n",
    "\n",
    "    :return: dict: A list of dictionaries containing the metrics for each event horizon.\n",
    "      - 'RMSE' (float): Root Mean Squared Error of the model's predictions.\n",
    "      - 'MAE' (float): Mean Absolute Error of the model's predictions.\n",
    "      - 'MAPE' (float): Mean Absolute Percentage Error of the model's predictions.\n",
    "      - 'RSQ' (float): R-squared value indicating the goodness of fit.\n",
    "      - 'Accuracy' (float): Accuracy of the model's directional predictions.\n",
    "      - 'Precision' (float): Precision of the model's directional predictions.\n",
    "      - 'Net Gain' (float): Net gain from simulated trading based on model predictions.\n",
    "      - 'Avg Profit' (float): Average profit per trade from simulated trading.\n",
    "      - 'Net Gain Percent' (float): Net gain percentage from simulated trading.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Preprocessing data for DeepAR\")\n",
    "    # minor preprocessing\n",
    "    data = day_average(data)\n",
    "    data = previous_day(data)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    original_df = data.copy()\n",
    "    data.drop(['Ticker', 'Exchange'], axis=1, inplace=True)\n",
    "    data.reset_index(inplace=True)\n",
    "\n",
    "    logging.info(\"Splitting data into training and validation sets\")\n",
    "    # train / test split\n",
    "    batch_size = 32\n",
    "    length = len(data)\n",
    "    n_batches = length // batch_size\n",
    "    train_n_batches = round(n_batches * 0.7)\n",
    "    test_n_batches = n_batches - train_n_batches\n",
    "    test_size = test_n_batches * batch_size\n",
    "    if test_n_batches % batch_size != 0:\n",
    "        test_size -= test_n_batches % batch_size\n",
    "    if test_size < max_prediction_length:\n",
    "        print(f\"Test size is too small for largest event horizon for: {data['Company'].iloc[0]}\")\n",
    "    split = train_n_batches * batch_size\n",
    "\n",
    "    # encoder errors occur when there isnt enough data\n",
    "    # I dont think this is the correct check but I'm still working on figuring out how to correctly check\n",
    "    if test_size < max_prediction_length + max_encoder_length:\n",
    "        max_encoder_length = 30  # this is the minimum encoder length, 6 weeks instead of 12 (5 day trade weeks)\n",
    "\n",
    "    logging.info(\"Number of batches: \" + str(n_batches))\n",
    "    logging.info(\"Train batches: \" + str(train_n_batches))\n",
    "    logging.info(\"Test batches: \" + str(test_n_batches))\n",
    "    logging.info(\"Max encoder length: \" + str(max_encoder_length))\n",
    "\n",
    "    logging.info(\"Creating TimeSeriesDataSet\")\n",
    "    # create training and validation sets and dataloaders\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.index < split],\n",
    "        time_idx=\"index\",\n",
    "        target=\"Close\",\n",
    "        group_ids=[\"Company\"],  # column name that identifies a time series\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=['Company'],\n",
    "        time_varying_known_reals=[\"Day Average\", \"Previous Close\", \"Previous Open\", \"Previous High\", \"Previous Low\",\n",
    "                                  \"Previous Volume\"],\n",
    "        time_varying_unknown_reals=[\"Close\"]  # this is the target\n",
    "    )\n",
    "    logging.info(f\"Dataset created for {data['Company'].iloc[0]}\")\n",
    "    logging.info(\"Creating dataloaders\")\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=3,\n",
    "                                              persistent_workers=True)\n",
    "    val_dataloader = TimeSeriesDataSet.from_dataset(training, data.iloc[split:split + test_size],\n",
    "                                                    min_prediction_idx=split,\n",
    "                                                    stop_randomization=True).to_dataloader(train=False,\n",
    "                                                                                           batch_size=batch_size,\n",
    "                                                                                           num_workers=3,\n",
    "                                                                                           persistent_workers=True)\n",
    "    logging.info(\"Creating model\")\n",
    "    # create model and train\n",
    "    loss = MultivariateDistributionLoss()\n",
    "    deepar = DeepAR.from_dataset(dataset=training,\n",
    "                                 learning_rate=5e-4,\n",
    "                                 hidden_size=12,\n",
    "                                 rnn_layers=3,\n",
    "                                 optimizer=\"AdamW\",\n",
    "                                 loss=loss\n",
    "                                 )\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='auto',\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=0.1,\n",
    "        enable_checkpointing=True\n",
    "    )\n",
    "\n",
    "    logging.info(\"Fitting model\")\n",
    "    trainer.fit(\n",
    "        deepar,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader\n",
    "    )\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    best_deepar = DeepAR.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    logging.info(\"Generating predictions\")\n",
    "    # generating predictions for test set\n",
    "    predict_df = data.iloc[split:]\n",
    "    predictions = best_deepar.predict(predict_df).cpu().numpy().reshape(-1)\n",
    "\n",
    "    # evaluation for each event horizon\n",
    "    event_horizons = [1, 5, 10, 20, 50, 100]\n",
    "    metrics = []  # this will be a list of dictionaries containing the metrics for each event horizon\n",
    "    try:\n",
    "        for horizon in event_horizons:\n",
    "            logging.info(\"Calculating metrics for event horizon: \" + str(horizon))\n",
    "            val_data = data.iloc[split:split + horizon]\n",
    "\n",
    "            val_data['Predicted Close'] = predictions[:horizon]\n",
    "            metrics_dict = {}\n",
    "            rmse, mae, mape, rsq, ev = calculate_standard_metrics(val_data['Close'], val_data['Predicted Close'])\n",
    "            metrics_dict['RMSE'] = rmse\n",
    "            metrics_dict['MAE'] = mae\n",
    "            metrics_dict['MAPE'] = mape\n",
    "            metrics_dict['RSQ'] = rsq\n",
    "\n",
    "            direction_df = direction_evaluation(val_data, 'Close', 'Predicted Close')\n",
    "            accuracy, precision = calculate_accuracy(direction_df, 'Close', 'Predicted Close')\n",
    "            metrics_dict['Accuracy'] = accuracy\n",
    "            metrics_dict['Precision'] = precision\n",
    "\n",
    "            net_gain, avg_profit, net_gain_percent = simulate_trading(direction_df, 10000)\n",
    "            metrics_dict['Net Gain'] = net_gain\n",
    "            metrics_dict['Avg Profit'] = avg_profit\n",
    "            metrics_dict['Net Gain Percent'] = net_gain_percent\n",
    "            metrics.append(metrics_dict)\n",
    "\n",
    "        horizons_metrics_list = []\n",
    "        for i in range(len(event_horizons)):\n",
    "            row = {\n",
    "                'Company': original_df['Company'][0],\n",
    "                'Ticker': original_df['Ticker'][0],\n",
    "                'Exchange': original_df['Exchange'][0],\n",
    "                'Event Horizon': event_horizons[i],\n",
    "                'RMSE': metrics[i]['RMSE'],\n",
    "                'MAE': metrics[i]['MAE'],\n",
    "                'MAPE': metrics[i]['MAPE'],\n",
    "                'RSQ': metrics[i]['RSQ'],\n",
    "                'Accuracy': metrics[i]['Accuracy'],\n",
    "                'Precision': metrics[i]['Precision'],\n",
    "                'Net Gain': metrics[i]['Net Gain'],\n",
    "                'Avg Profit': metrics[i]['Avg Profit'],\n",
    "                'Net Gain Percent': metrics[i]['Net Gain Percent']\n",
    "            }\n",
    "            horizons_metrics_list.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} when calculating metrics for: {data['Company'].iloc[0]}\")\n",
    "\n",
    "    return horizons_metrics_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start Here to Run DeepAR\n",
    "Run ll of the above cells,\n",
    "Then follow the comments for the Load or Query data section."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load or Query Data:\n",
    "To load or query data, you will need to upload either the given stock list file or the stock_data folder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run this cell for querying using the stock list file\n",
    "stocks_list = pd.read_csv('Uranium Company Master List.csv')\n",
    "periods = 365*5 # 3 years, however, some stocks only have 2022 to the present (was 5 years, need to conserve memory on my machine)\n",
    "data_list = query_yf_list(stocks_list, periods)\n",
    "for data in data_list:\n",
    "    data.reset_index(inplace=True)  # must have an integer index for Prophet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run this cell for loading data from a folder mounted to your Google Drive environment (optional)\n",
    "data_folder = 'stock_data'\n",
    "data_list = read_csvs_from_folder(data_folder, list=True)\n",
    "urnm_data_folder = 'stock_data/SP Uranium ETF'\n",
    "urnm_data = read_csvs_from_folder(urnm_data_folder, list=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run this to query data for the SP 500 Uranium ETF\n",
    "stock = \"Sprott Uranium Miners ETF\", \"URNM\", \"XNYS\"\n",
    "period = 365*5\n",
    "urnm_data = query_yf(stock, period)\n",
    "urnm_data.reset_index(inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run this cell to save the data to a folder (optional)\n",
    "save_stocks_to_csv(data_list, folder_path='stock_data')\n",
    "urnm_data.to_csv(os.path.join(\"stock_data/SP Uranium ETF\", \"Sprott Uranium Miners ETF data.csv\"), index='Date')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running DeepAR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_encoder_length = 60\n",
    "max_prediction_length = 100\n",
    "for data in data_list:\n",
    "    horizons_metrics_list = run_deepar(data, max_prediction_length, max_encoder_length)\n",
    "    horizons_df = pd.DataFrame(horizons_metrics_list)\n",
    "    company_name = horizons_df['Company'][0]\n",
    "\n",
    "    results_dir = os.path.join(os.getcwd(), 'results')\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    deepar_dir = os.path.join(os.getcwd(), 'results/DeepAR')\n",
    "    if not os.path.exists(deepar_dir):\n",
    "        os.makedirs(deepar_dir)\n",
    "\n",
    "    file_path = os.path.join(deepar_dir, f\"DeepAR Metrics-{company_name}.csv\")\n",
    "    horizons_df.to_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
